{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Silvmike/LLM-Engineering-Essentials/blob/main/topic2/2.3_intro_to_llm_reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm506vpf9u9b"
      },
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon. [Subscribe to stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)\n",
        "\n",
        "# 2.3. Intro to LLM Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7tZwYDa25qm"
      },
      "source": [
        "LLMs' reasoning ability has long fascinated people, drawing attention for its practical implications and sparking intriguing theoretical questions. In this series of notebooks, we'll explore what reasoning brings to the table, how our understanding of it has evolved over time, and why it's been generating so much hype lately.\n",
        "\n",
        "The plan is:\n",
        "\n",
        "* **R1. Intro to LLM Reasoning**. What LLM reasoning is, for which tasks it is useful (and for which it is not). Have LLMs really learnt to \"think\" like humans (not exactly).\n",
        "* **R2. Inference-time compute**. How to make an LLM smarter with orchestration.\n",
        "* **R3. Establishing non-linear reasoning capabilities**: how DeepSeek R1 was trained and some other approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6lr3K4OXbAb"
      },
      "source": [
        "## Getting ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqCgRtIRIcN3"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRpRGdl5IdJZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ElsBJ68uacB"
      },
      "source": [
        "We'll be calling APIs quite often in this notebook, so let's define a shortcut fuction to avoid repeating all the code. Also, we'll prettify the output in such a way that it can be viewed without scrolling right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTlC-5omIVOO"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Nebius uses the same OpenAI() class, but with additional details\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_8b_model,\n",
        "                    prettify=True,\n",
        "                    temperature=0.6) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcema7Jxz7nG"
      },
      "source": [
        "# What is LLM Reasoning\n",
        "\n",
        "There are several ways in which LLMs may respond to your queries.\n",
        "\n",
        "Sometimes, they just give a direct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e52qmMO6u5Et"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Who is the main character of the Mistborn trilogy?\"\"\",\n",
        "                         model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC5BCYNCu5Pc"
      },
      "source": [
        "However, if you give a math task to one of today's LLMs, you'll notice that it doesn't just spit out an answer - it provides a full solution instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-Fg4Pjw0oaF"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"In the fantasy world of Xu, they have unique math system:\n",
        "- \"a + b\" means min(a,b)\n",
        "- \"a*b\" means a + b\n",
        "Solve the equation x*x + 2*x + 1 = 0\"\"\",\n",
        "                         model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9gMHoIm-Axv"
      },
      "source": [
        "**Note**. If you're interested in math, Xu's math system is actually called [Tropical Geometry](https://en.wikipedia.org/wiki/Tropical_geometry)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-zY_qkq0omG"
      },
      "source": [
        "Back in 2023, LLM researchers and users noticed that, at least for mathematical tasks, LLMs produced more accurate answers when generating a full solution rather than simply providing a direct answer. At that time, LLMs didn't always automatically write solutions, and you might have needed to prompt them with `\"Take a deep breath and solve this problem step by step.\"`\n",
        "\n",
        "Getting detailed, step-by-step solutions was an exciting development. Such solutions became known as **Chains of Thought (CoT)**, and wide discussion began about **LLM reasoning capabilities**.\n",
        "\n",
        "Today math reasoning is a standard out-of-the box capability of the absolute majority of LLMs. But now, a more powerful paradigm is gaining popularity: **non-linear reasoning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssjegCLy28N2"
      },
      "source": [
        "### Non-linear reasoning\n",
        "\n",
        "Soon after Chains of Thoughts (CoT) emerged, it became clear that they are not enough. Indeed, the CoT paradigm assumes that an LLM is able to generate the correct solution from the first attempt, while the human way of thinking involves checking several ideas, experimenting, self-criticizing, and backtracking before generating the final solution\n",
        "\n",
        "So, the **non-linear reasoning** approach was born.\n",
        "\n",
        "For a couple of years, non-linear reasoning was established with help of orchestration. Mechanisms such as [Tree of Thoughts](https://arxiv.org/pdf/2305.10601) or [Graph of Thoughts](https://arxiv.org/pdf/2308.09687) were suggested for solving complex problems. We'll discuss them in detail in the **Inference-time compute** notebook.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WZWjI7aY3Vu0zEsAO8u7R73iwsC6KJeq\" width=600 />\n",
        "\n",
        "[Source](https://arxiv.org/pdf/2308.09687)\n",
        "</center>\n",
        "\n",
        "A general idea of such approaches is to generate a solution step by step (one prompt = one logical step, unlike CoT), while exploring several reasoning paths and somehow scoring individual steps or whole branches to eventually select the optimal reasoning path.\n",
        "\n",
        "However, as often happens in Machine Learning, orchestration strategies eventually give way to end-to-end ones. And it seems that we're almost at the point where LLMs are able to perform non-linear reasoning on their own.\n",
        "\n",
        "To illustrate this, let's compare outputs of Phi-4, Llama, and DeepSeek R1, which is a top-trend non-linear reasoning model.\n",
        "\n",
        "**The task is:** Imagine that my binary classifier got recall 0.8 on a dataset with balanced classes (same number of class 0 and class 1 objects). What could be its minimal and maximal precision?\n",
        "\n",
        "<details>\n",
        "    <summary> Click to see the solution </summary>\n",
        "\n",
        "Let $x$ be the number of class 1 objects. Than recall 0.8 means that 80% of them are classified as class 1 (that's TN) and 20% as class 0 (that's FN). Let's populate the magic table:\n",
        "\n",
        "|                | Classified as class 1 | Classified as class 0 |\n",
        "| :---------------- | :------: | ----: |\n",
        "| Class 1        |   $0.8x$   | $0.2x$ |\n",
        "| Class 0           |   ???   | ??? |\n",
        "\n",
        "Since the dataset is balanced, Class 0 also contains $x$ elements. So, we get some\n",
        "\n",
        "|                | Classified as class 1 | Classified as class 0 |\n",
        "| :---------------- | :------: | ----: |\n",
        "| Class 1        |   $0.8x$   | $0.2x$ |\n",
        "| Class 0           |   $\\alpha x$   | $(1 - \\alpha)x$ |\n",
        "\n",
        "where $0\\leqslant \\alpha \\leqslant 1$ (and that's all we know about $\\alpha$. Now, the precision is\n",
        "$$\\frac{0.8x}{0.8x + \\alpha x} = \\frac{0.8}{0.8 + \\alpha},\\quad 0\\leqslant\\alpha\\leqslant1$$\n",
        "\n",
        "Now, some math establishes the answer:\n",
        "$$0\\leqslant\\alpha\\leqslant1 \\Rightarrow 0.8\\leqslant 0.8 + \\alpha\\leqslant 1.8 \\Rightarrow$$\n",
        "\n",
        "$$\\Rightarrow\\frac1{1.8}\\leqslant\\frac1{0.8 + \\alpha} \\leqslant \\frac1{0.8}\n",
        "\\Rightarrow \\frac49=\\frac{0.8}{1.8}\\leqslant\\frac{0.8}{0.8 + \\alpha} \\leqslant \\frac{0.8}{0.8} = 1$$\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aPS-4jdgnK3r"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Imagine that my binary classifier got recall 0.8 on a dataset with balanced classes (same number of class 0 and class 1 objects).\n",
        "What could be its minimal and maximal precision?\"\"\",\n",
        "                model=\"microsoft/phi-4\",\n",
        "                system_prompt=None,\n",
        "                max_tokens=4096)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxRtTFUiO-C"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Imagine that my binary classifier got recall 0.8 on a dataset with balanced classes (same number of class 0 and class 1 objects).\n",
        "What could be its minimal and maximal precision?\"\"\",\n",
        "                model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                system_prompt=None,\n",
        "                max_tokens=4096)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPkDmHgUp3NH"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Imagine that my binary classifier got recall 0.8 on a dataset with balanced classes (same number of class 0 and class 1 objects).\n",
        "What could be its minimal and maximal precision?\"\"\",\n",
        "                model=\"deepseek-ai/DeepSeek-R1\",\n",
        "                system_prompt=None,\n",
        "                max_tokens=4096)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MxNXpvt6RF"
      },
      "source": [
        "Let's briefly analyze the outputs.\n",
        "\n",
        "* **Phi-4** by Microsoft and **Llama-3.1-70B** provide typical Chain-of-Thoughts solutions.\n",
        "* **DeepSeek R1**'s reasoning features several backtracking episodes which definitely characterize it as non-linear:\n",
        "\n",
        "  ```\n",
        "  Okay, let's try to figure out the minimal and maximal precision for a binary\n",
        "classifier....\n",
        "\n",
        "  But wait, the classifier's predictions are also influenced by how many class 0\n",
        "samples it classifies correctly or incorrectly...\n",
        "\n",
        "  Wait, but can the classifier actually have FP=50?..\n",
        "\n",
        "  Alternatively, perhaps there's a confusion matrix here...\n",
        "\n",
        "  Wait, but wait, when FP is zero, that means the classifier predicted all class\n",
        "0 samples as class 0.... --> FINAL ANSWER\n",
        "  ```\n",
        "\n",
        "Note also that **DeepSeek R1** outputs reasoning in `<think>...</think>`, and only after that it gives the final solution.\n",
        "\n",
        "\n",
        "There are already quite a lot non-linear reasoning models, both proprietary and open source. They include:\n",
        "  - OpenAI's **o1** and **o3**.\n",
        "  - Anthropic's **Claude 3.7 Sonnet**\n",
        "  - Google's Experimental Thinking **Gemini 1.5** and **2**\n",
        "  - **Grok 3** by X\n",
        "  - **DeepSeek R1** which is the open source model that produced lots of hype due several reasons: not only its low training and inference cost were scandalously low, but also this model is open source, with more or less clear and unexpected training strategy which we'll discuss in the 3rd of the reasoning-related notebooks.\n",
        "  - **QWQ** by Alibaba.\n",
        "\n",
        "In this notebook and in the next two, we'll investigate non-linear reasoning, bith native and orchestrated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_vFtRn0h0Vg"
      },
      "source": [
        "**Note**. We used empty `system_prompt` for a reason. It didn't affect a Machine Learning Theory task, because LLMs don't seem to recognize it as a math task, but notice the difference in output patterns for a math task with and without the helpful assistant system prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYQUL2Srh5R-"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Inside a circle, two parallel chords are 6 units apart. One chord has length 14 and the other has length 10. Find the radius of the circle.\"\"\",\n",
        "                model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                system_prompt=\"You are a helpful assistant\",\n",
        "                max_tokens=4096)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9a3xkb8hFT7"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Inside a circle, two parallel chords are 6 units apart. One chord has length 14 and the other has length 10. Find the radius of the circle.\"\"\",\n",
        "                model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                system_prompt=None,\n",
        "                max_tokens=4096)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSjdWyKmnDC"
      },
      "source": [
        "# When is reasoning useful?\n",
        "\n",
        "It is clear that reasoning significantly boosts LLM performance on certain tasks. LLMs incorporating non-linear reasoning have achieved remarkable breakthroughs on several benchmarks once thought to be beyond the reach of AI. In particular, the emergence of **o3** has shaken two of the most challenging benchmarks: [ARC-AGI](https://arcprize.org/arc) and [FrontierMath](https://epoch.ai/frontiermath).\n",
        "\n",
        "<center>\n",
        "<img src=\"https://pbs.twimg.com/media/Gi03TkpbMAAun6w?format=jpg&name=large\" width=600 />\n",
        "\n",
        "[Source](https://x.com/andrewwhite01/status/1886225029006062051)\n",
        "</center>\n",
        "\n",
        "However, while reasoning is great in math tasks, in some other cases it may be useless or even harmful. [To CoT or not to CoT](https://arxiv.org/pdf/2409.12183) is one of the papers investigating that. The authors did a number of experiments on different tasks and came to a conclusion that\n",
        "\n",
        "* CoT is (non-surprisingly) quite useful in math tasks and tasks involving symbolic computations.\n",
        "* CoT is not useful in tasks that check factual knowledge or involve commonsense reasoning.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1pQYlQOrOLPyv9EgGBdQTVivNtR5WUnnn\" width=600 />\n",
        "\n",
        "[Source](https://arxiv.org/pdf/2409.12183)\n",
        "</center>\n",
        "\n",
        "Another insightful papers showing the downsides of CoT, this time in visual tasks, is [Mind your Step (by Step): Chain-of-Thought can reduce performance on tasks where thinking makes humans worse](https://arxiv.org/pdf/2410.21333). The name is quite self-explanatory. Among others, the authors consider facial recognition - both humans and Multimodal LLMs do it better if not prompted to perform reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utWRrw0UFfBh"
      },
      "source": [
        "Let's also run some experiments!\n",
        "\n",
        "We'll use the [MMLU benchmark](https://huggingface.co/datasets/cais/mmlu), which contains tasks in many areas, from International Law to Abstract Algebra. Let's check the scores of **Llama-3.1-8B**, **Llama-3.1-70B**, and **Qwen-2.5-32B** in **High School Math** and **High School History** in two modes:\n",
        "\n",
        "* First, when the LLM is prompted to only give the answer,\n",
        "* Second, when the LLM is prompted to perform a step-by-step reasoning before giving the final answer.\n",
        "\n",
        "We'll create a `MMLUEvaluator` class to steamline evaluation. If you create a similar evaluation class, be careful with the `max_tokens` parameter in the `answer_with_llm` function. It should be large enough; otherwise solutions may be cut short, resulting in surptisingly low accuracy with CoT. This is especially true for non-linear reasoning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GZX74Z03vXH"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf6nXshN3vZk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        if not prompt:\n",
        "            self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "        else:\n",
        "            self.prompt = prompt\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.strip('.')[-1]\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                client=client, model=model,\n",
        "                system_prompt=self.system_prompt,\n",
        "                max_tokens=4096,\n",
        "                temperature=0.\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client: OpenAI=nebius_client,\n",
        "                       model: str=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions: int = 50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zdGhOfcHJ_Q"
      },
      "source": [
        "We'll create different prompts for No-CoT and CoT scenarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKPJ06xOHPYw"
      },
      "outputs": [],
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "prompts = {\"No CoT\": \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "Output only the correct answer label, one of the letters A, B, C, or D.\n",
        "Only output one letter - A, B, C, or D.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\",\n",
        "\"With CoT\": \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPj0V6IRZ255"
      },
      "source": [
        "Finally, let's look at the numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03pQmyP9cB7H"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "for topic in [\"high_school_world_history\", \"high_school_mathematics\"]:\n",
        "    for mode in [\"No CoT\", \"With CoT\"]:\n",
        "\n",
        "        evaluator = MMLUEvaluator(topic=topic,\n",
        "                          prompt=prompts[mode])\n",
        "\n",
        "        results = evaluator.run_evaluation(\n",
        "            client=client,\n",
        "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "            n_questions=50\n",
        "            )\n",
        "        evaluation_results[(topic, mode)] = results[\"accuracy\"]\n",
        "        print(f\"For topic {topic}, mode {mode}\")\n",
        "        print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWb6Dcx74DBP"
      },
      "outputs": [],
      "source": [
        "for topic in [\"high_school_world_history\", \"high_school_mathematics\"]:\n",
        "    for mode in [\"No CoT\", \"With CoT\"]:\n",
        "\n",
        "        evaluator = MMLUEvaluator(topic=topic,\n",
        "                          prompt=prompts[mode])\n",
        "\n",
        "        results = evaluator.run_evaluation(\n",
        "            client=client,\n",
        "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "            n_questions=50\n",
        "            )\n",
        "        evaluation_results[(topic, mode)] = results[\"accuracy\"]\n",
        "        print(f\"For topic {topic}, mode {mode}\")\n",
        "        print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mArpi1MI1vWZ"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "for topic in [\"high_school_world_history\", \"high_school_mathematics\"]:\n",
        "    for mode in [\"No CoT\", \"With CoT\"]:\n",
        "\n",
        "        evaluator = MMLUEvaluator(topic=topic,\n",
        "                          prompt=prompts[mode])\n",
        "\n",
        "        results = evaluator.run_evaluation(\n",
        "            client=client,\n",
        "            model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
        "            n_questions=50\n",
        "            )\n",
        "        evaluation_results[(topic, mode)] = results[\"accuracy\"]\n",
        "        print(f\"For topic {topic}, mode {mode}\")\n",
        "        print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pji7_CZNXtpZ"
      },
      "source": [
        "As you see, while the effect of CoT in **High School Math** is very significant, for **Hight School World History** it doesn't improve anything, and in some experiment launches it may even slightly spoil the result.\n",
        "\n",
        "Of course, one experiment is not enought to *establish* a law, but it's a good illustration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygZQOw303Gxm"
      },
      "source": [
        "Another downside of reasoning models is that their output tends to get bloated even for relatively simple tasks for which other models would give a shorter and straightforward solution. This, of course, doesn't help the fact that they are generally slow and expensive.\n",
        "\n",
        "A couple of examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Y4ctO33h7V"
      },
      "source": [
        "Who is the main character of the Mistborn trilogy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNWqNu5q3mW-"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Who is the main character of the Mistborn trilogy?\"\"\",\n",
        "                         model=\"deepseek-ai/DeepSeek-R1\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKW4P55r3mxU"
      },
      "outputs": [],
      "source": [
        "result = answer_with_llm(\"\"\"Jack the Sparrow has 12 sailors and Davy Jones has 125 sailors.\n",
        "If they join their crews, how many sailors will they have together?\"\"\",\n",
        "                         model=\"deepseek-ai/DeepSeek-R1\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzEj8o4_4iDw"
      },
      "source": [
        "I can't help thinking that **R1** is making fun of me with the sailor example... It even failed to answer with a reasonable `max_tokens` limit. But don't think that **R1** will be as indecisive with arithmetic in more challenging problems. It's just the LLM is trained to produced long solutions. We'll yet return to this idea in our third notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFMF-71KHrV"
      },
      "source": [
        "# Why LLM Reasoning works?\n",
        "\n",
        "It's very tempting to say \"Because LLMs learn to reason like humans\". But is it really so? In this section we'll discuss several papers investigating this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDlkexrFK2QL"
      },
      "source": [
        "## Let's reason dot by dot\n",
        "\n",
        "The authors of the [Let's reason dot by dot](https://arxiv.org/pdf/2404.15758) paper conducted a curious experiment. They fine tuned an LLM to output dots (literally `\".\"` tokens) instead of actual reasoning tokens:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1G9g8zDC1wsr9YvdXC49c2nebb3Og38by\" width=400 />\n",
        "</center>\n",
        "\n",
        "You'd expect that such a \"stupid\", dot-minded model won't be good at anything, but in reality it behaves significantly better than the same LLM fine tuned for giving an immediate answer without reasoning.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1VCz8aDXVckgG5Eh08xMNjoFbUROvBX7i\" width=600 />\n",
        "</center>\n",
        "\n",
        "<details>\n",
        "<summary> Click to see the description of the 3SUM task\n",
        "</summary>\n",
        "\n",
        "We'll have to explain a few things first. $\\mathbb{Z}_{10}$ is the group of **remainders modulo $10$**, that is $\\mathbb{Z}_{10} = \\{\\overline{0}, \\overline{1},\\ldots, \\overline{9}\\}$, where addition is performed modulo $10$. For example,\n",
        "$$\\overline{3} + \\overline{4} = \\overline{7},$$\n",
        "$$\\overline{7} + \\overline{8} = \\overline{5},$$\n",
        "because $7 + 8 = 15 \\equiv 5\\, (mod\\, 10)$ (meaning: 15 and 5 give same remainders when divided by 10: `15%10 = 5%10`).\n",
        "Now, in the 3SUM task we're given a set $(x_0,\\ldots,x_n)$ of pairs $x_i = (x_i', x_i'')$, where $x_i', x_i''\\in\\mathbb{Z}_{10}$.\n",
        "\n",
        "\n",
        "The 3SUM task here is determining whether there are 3 pairs $x_i,x_j,x_k$ among $(x_0,\\ldots,x_n)\\in\\mathbb{Z}_{10}\\times\\mathbb{Z}_10$. The task is to determine whether there are three pairs $x_i, x_j, x_k$ such that\n",
        "$$x_i + x_j + x_k = 0\\,(mod\\,10).$$\n",
        "\n",
        "For example, if we have pairs\n",
        "$$x_0 = (\\overline{5}, \\overline{4}),$$\n",
        "$$x_1 = (\\overline{7}, \\overline{3}),$$\n",
        "$$x_2 = (\\overline{0}, \\overline{1}),$$\n",
        "$$x_3 = (\\overline{8}, \\overline{3}),$$\n",
        "then\n",
        "$$x_0 + x_1 + x_3 = (\\overline{5} + \\overline{7} + \\overline{8},\\\n",
        "\\overline{4} + \\overline{3} + \\overline{3}) = (\\overline{0}, \\overline{0}),$$\n",
        "because\n",
        "$$5 + 7 + 8 = 20 \\equiv 0\\, (mod\\, 10)$$\n",
        "$$4 + 3 + 3 = 10 \\equiv 0\\, (mod\\, 10)$$\n",
        "\n",
        "This task is good, because you can't solve it in one pass over the dataset, it really requires some computations.\n",
        "\n",
        "The figure above shows that, with $n$ growing, an immediatly-answering model performes worse and worse, while the dot-reasoning keeps the same quality.\n",
        "\n",
        "</details>\n",
        "\n",
        "**Takeaways**. It turns out that it's not absolutely necessary for an LLM to output human-readable reasoning in order to solve problems. It beckons the hypothesis that the real \"thought\" process happens somewhere in the LLM's bowels, in the realm of vectors and matrices. If that's true, the neat verbal reasoning might be more of a byproduct. A bit later, we'll discuss a paper which leverages this, taking LLM reasoning even further from human readibility. But now, let's do an experiment of our own!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctQ6R-e0u3uP"
      },
      "source": [
        "Let's run a very naive experiment. In the original paper, the authors fine tuned the model, but we'll just prompt **Llama-3.1-8B** to output dots instead of the actual reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dle1BgHLvYxz"
      },
      "outputs": [],
      "source": [
        "dot_prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "Instead of reasoning, output dots. Then, after #ANSWER: only output one of the letters - A, B, C, or D, the correct answer label.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "dot_evaluator = MMLUEvaluator(topic=\"high_school_mathematics\",\n",
        "                          prompt=dot_prompt)\n",
        "\n",
        "dot_results = dot_evaluator.run_evaluation(\n",
        "    client=client,\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    n_questions=50)\n",
        "print(f'\\nAccuracy: {dot_results[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huGSrMhf0eRZ"
      },
      "source": [
        "Of course, the accuracy is much lower than with CoT, but still it's quite stably 10% higher than with immediate answering! (26% -> 36%)\n",
        "\n",
        "Let's also check that the model really outputs dots. (It does!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdAQNblwwAQ7"
      },
      "outputs": [],
      "source": [
        "dot_results['evaluation_log'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvIjpW6A5JHK"
      },
      "outputs": [],
      "source": [
        "dot_results['evaluation_log'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5CHm8-h5CWX"
      },
      "source": [
        "There seem to be no connection between the number of dots and the lengths of the actual CoT solutions. In our experiments, the number of dots remained stable for each problem, which is probably not very surprising given that the temperature is low, but still curious.\n",
        "\n",
        "Please, also note that this experiment, although fun, doesn't reproduce well with other LLMs. So, there is something peculiar about Llama-3.1-8B here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXsYwZCFRvPi"
      },
      "source": [
        "# Coconut (Chain of Continuous Thought)\n",
        "\n",
        "In the [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/pdf/2412.06769) paper by Meta, the authors try to totally get rid of human-readable reasoning.\n",
        "\n",
        "To understand their idea, let's recall a couple of things about LLM architecture and generation process.\n",
        "\n",
        "**1. Token embeddings**. As any neural networks, LLMs can only have vectors as inputs. So, instead of tokens themselves, LLMs consume their **vector embeddings**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-j02KittQhnV-feGdsh7A1bv2aJzwGJz\" width=600 />\n",
        "</center>\n",
        "\n",
        "All the embeddings of a prompt go through a number of transformer blocks to become transformer **output** vectors (in grey). The last one of those is additionally passed though an **LM head** (also known as an **unembedding layer**) which is a multiclass classifier that predicts next token probabilities.\n",
        "\n",
        "The newly generated token is then appended to the prompt and passed back to the LLM for it to produce the next one:\n",
        "\n",
        "* ...\n",
        "* $x_1x_2\\ldots x_i\\phantom{x_{i+1}}\\longrightarrow x_{i+1}$\n",
        "* $x_1x_2\\ldots x_ix_{i+1}\\longrightarrow x_{i+2}$\n",
        "* ...\n",
        "\n",
        "COCONUT suggests getting rid of completion tokens that correspond to the reasoning part of the completion. Instead, **the last transformer output vector (grey) becomes the new blue, i.e. is appended to the transformer input as the next embedding vector**:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-MJLKdp2H443HoEVrHptbpcwE32j6XDe\" width=800 />\n",
        "\n",
        "[Source](https://arxiv.org/pdf/2412.06769)\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Zeogn1nBiE"
      },
      "source": [
        "# Ready for more?\n",
        "\n",
        "This notebook is part of the larger free course — **LLM Engineering Essentials** — where you’ll go even further in your learning and build a service for creating smart, human-like NPCs.\n",
        "\n",
        "🎓 New materials are coming soon. Click the link below to subscribe for updates and make sure you don’t miss anything:\n",
        "\n",
        "[Stay updated](https://academy.nebius.com/llm-engineering-essentials/update/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894uX3nFxtwM"
      },
      "source": [
        "Of course, an LLM should be trained for such a mode of operation, and the authors do it in several stages, starting with a natural language reasoning and replacing them step by step with the steps of the new procedure (**“latent thoughts”**).\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1u0jW8ALA6v4RUdDuNuxlQoh0mwQ5HX3A\" width=800 />\n",
        "\n",
        "[Source](https://arxiv.org/pdf/2412.06769)\n",
        "</center>\n",
        "\n",
        "They also introduce additional <bot> and <eot> (“beginning/end of thought”) tokens to indicate that it's time to start a thought or continue producing natural language outputs.\n",
        "\n",
        "During the training process, the loss is masked on both questions and latent thoughts. This means that the latent thoughts aren't trained to repeat the initial natural language reasoning; only to facilitate further reasoning. Therefore, it's possible for the LLM to learn more effective representations of reasoning steps compared to human language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqZVdvqPxRpA"
      },
      "source": [
        "Because latent thoughts aren't mapped to tokens, on inference the `<eot>` token isn't spawned naturally. Instead, the authors suggest either of these two ways of escaping latent reasoning:\n",
        "\n",
        "1.\ttraining a binary classifier on latent thoughts to predict `<eot>`/not `<eot>`,\n",
        "2.\talways pad the latent thoughts to a constant length.\n",
        "\n",
        "The results are quite solid:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1u87jI_AStdWR0cXbvqcbniANF1OCsePg\" width=800 />\n",
        "\n",
        "[Source](https://arxiv.org/pdf/2412.06769)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAl1Qub81LjL"
      },
      "source": [
        "# Practice: Exploring non-linear reasoning\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/r.1_intro_to_llm_reasoning_solutions.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IM02EH_1dxJ"
      },
      "source": [
        "## Task 1. Mapping LLM \"thoughts\"\n",
        "\n",
        "In this task, we'll look closer at \"thinking patters\" of LLMs:\n",
        "\n",
        "- We'll look closer at an LLM's \"tree of thoughts\",\n",
        "- We'll investigate how long the typical thoughts are,\n",
        "- We'll explore the \"underthinking\" phenomenon.\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/r.1_intro_to_llm_reasoning.ipynb).\n",
        "\n",
        "To have something to experiment with, we'll run evaluation of **QwQ-32B-Preview** on a subset of [MATH benchmark](https://huggingface.co/datasets/nlile/hendrycks-MATH-benchmark). This benchmark is relatively challenging, but to a reasonable extent. It's not [FrontierMath](https://epoch.ai/frontiermath) :)\n",
        "\n",
        "We'll take the first 50 problems that satisfy two following conditions:\n",
        "\n",
        "- Their answer is either straightforwardly converted to `float`, or it's a simple Latex-formatted fraction, like `\\frac{2}{3}`.\n",
        "- Their \"level\" is either 4 ot 5 (more challenge!).\n",
        "\n",
        "**Note** You can use **deepseek-ai/DeepSeek-R1**, if you want, but it will generate solutions *very* slowly (not mentioning the cost).\n",
        "\n",
        "Also, if you don't want to run the evaluation of **QWQ** on your own, you may download the `qwq_results.pkl` file from Google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPEXmWjW2TNN"
      },
      "outputs": [],
      "source": [
        "!gdown 1_hEX_h7fj6FXH3lG5AMthJjiK1JwAU9J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eQXrlRi9Cj9"
      },
      "source": [
        "### Evaluating QWQ on MATH Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V62K3KmJ7sNm"
      },
      "source": [
        "If you're in, let's create the evaluator. And we'll start by data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQL9RmxsYRSy"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7B4oUe6YRVL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('nlile/hendrycks-MATH-benchmark', split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyOS3mGNYmKu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def conver_string_to_number(s):\n",
        "    \"\"\"\n",
        "    Checks if a string is a number or a fraction and computes the fraction if applicable.\n",
        "\n",
        "    Args:\n",
        "        s: The input string.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the number or fraction, or None if the string is invalid.\n",
        "    \"\"\"\n",
        "    if \"_\" in s:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return float(s)  # Try converting to a float directly\n",
        "    except ValueError:\n",
        "        match = re.match(r\"\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}\", s)\n",
        "        if match:\n",
        "            numerator = int(match.group(1))\n",
        "            denominator = int(match.group(2))\n",
        "            if denominator != 0:\n",
        "                return numerator / denominator\n",
        "            else:\n",
        "                return None  # Handle division by zero\n",
        "        else:\n",
        "            return None  # String is not a number or a valid fraction\n",
        "\n",
        "# Example usage\n",
        "strings = [\"123\", \"\\\\frac{1}{2}\", \"\\\\frac{3}{0}\", \"abc\", \"\\\\frac{4}{5}\"]\n",
        "for s in strings:\n",
        "    result = conver_string_to_number(s)\n",
        "    if result is not None:\n",
        "        print(f\"'{s}' is a valid number or fraction. Result: {result}\")\n",
        "    else:\n",
        "        print(f\"'{s}' is not a valid number or fraction.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2W93rs6iY8_u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(ds)\n",
        "\n",
        "df['num_answer'] = df['answer'].apply(conver_string_to_number)\n",
        "df['valid_answer'] = df['num_answer'].notna()\n",
        "\n",
        "# Select the first 50 rows where the 'answer' column passes the check\n",
        "selected_rows = df[(df['valid_answer']) & (df['level'] >= 4)].head(50)\n",
        "selected_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1Bz-QEP8GO8"
      },
      "source": [
        "The evaluator itself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFgDjWzaaAUa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def find_boxed_content(text):\n",
        "    matches = re.findall(r\"boxed\\{(.*?)\\}\", text)\n",
        "    try:\n",
        "        return conver_string_to_number(matches[-1])\n",
        "    except:\n",
        "        try:\n",
        "            return conver_string_to_number(text.split(\"\\n\")[-1].split(\" \")[-1].strip(\".;$\"))\n",
        "        except:\n",
        "            print(f\"\"\"Wrong format in:\n",
        "                {text.split()[-1]}\"\"\")\n",
        "            return None\n",
        "\n",
        "class MATHEvaluator:\n",
        "    def __init__(self, system_prompt: str = \"You are a helpful assistant.\",\n",
        "                 prompt: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the MATH evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "        \"\"\"\n",
        "\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "        self.prompt = \"\"\"{question}\"\"\"\n",
        "\n",
        "        self.questions, self.answers = selected_rows[\"problem\"].to_list(), selected_rows[\"num_answer\"].to_list()\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer (an float)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            # answer = float(solution.split(\"\\n\").split(\" \")[1].strip(\".;)\"))\n",
        "            answer = find_boxed_content(solution)\n",
        "        except:\n",
        "            answer = None\n",
        "        # print(solution.split(\"\\n\")[-1])\n",
        "        # print(answer)\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str,\n",
        "                                 correct_answer: float,\n",
        "                                 client, model, max_tokens,\n",
        "                                 temperature=None) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    question=question\n",
        "                ),\n",
        "                client=client, model=model,\n",
        "                system_prompt=self.system_prompt,\n",
        "                max_tokens=max_tokens\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            if answer:\n",
        "                is_correct = np.abs(answer - correct_answer) < 1e-10\n",
        "            else:\n",
        "                is_correct = False\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client : OpenAI, model : str,\n",
        "                       n_questions=50, max_tokens=8192, temperature=0.) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "        correct_format_count = 0\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "            if answer:\n",
        "                correct_format_count += 1\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'correct_format': not answer is None,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        format_correctness = correct_format_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'format_correctness': format_correctness,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfXAPAXYargI"
      },
      "outputs": [],
      "source": [
        "math_evaluator = MATHEvaluator(system_prompt=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiUM6YL3argI"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "qwq_results = math_evaluator.run_evaluation(\n",
        "    client=client,\n",
        "    model=\"Qwen/QwQ-32B-Preview\",\n",
        "    n_questions=None,\n",
        "    max_tokens=None\n",
        ")\n",
        "print(f'\\nAccuracy: {deepseek_results[\"accuracy\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRqbf9bS7-u1"
      },
      "source": [
        "Let's save the results to file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3xQLmco9JQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(qwq_results, open(\"qwq_results.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvsrI3l3Ye6v"
      },
      "source": [
        "Now, you can load the file even if you didn't create it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNDZKkWco9Tb"
      },
      "outputs": [],
      "source": [
        "!gdown 1_hEX_h7fj6FXH3lG5AMthJjiK1JwAU9J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwMw22w-79BJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "qwq_results = pickle.load(open(\"qwq_results.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHTNC7g79Ji"
      },
      "source": [
        "### Analyzing thoughts\n",
        "\n",
        "We've prepared quite a large thought analysis and visualization script; so we decided not to include it here (please check it in github if you're curious). Here, we'll only download and import it from `thought_analysis.py`.\n",
        "\n",
        "A few words about what's happening in `thought_analysis.py`:\n",
        "\n",
        "- First of all, if the solution has `<think>...</think>` markup inside, only the fragment between them is extracted. (We're only interested in the \"internal\" thinking process.)\n",
        "- Then, solutions are divided into individial \"thoughts\" using the following heuristics:\n",
        "  - `Alternatively`, `Wait`, `But wait`, `But let me check again`, `But let's verify`, and similar phrases mark the starts of new \"thoughts\". Note that they are typical indications of backtracking and solution branching. There may be more, of course.\n",
        "  - Otherwise, a \"thought\" is a continuous range of paragraphs of length not less than `min_split_size` characters (we'll take `min_split_size=120`). Separate-line Latex formulas are always added to the previous thought.\n",
        "- For each \"thought\", its length in tokens is calculated. For that, we need to supply the right **tokenizer** which corresponds to the model which generated the solutions - in our case, **[QwQ-32B-Preview](https://huggingface.co/Qwen/QwQ-32B-Preview)**. And that's why you needed to create a **Hugging Face access token**. If you haven't done it yet, please register to HF, get the token and load it to colab in a `hf_access_token` file.\n",
        "\n",
        "  If you're ardently against registering to Hugging Face, you can supply `None` as `tokenizer`, but in this case you won't get correct token length of individual \"thoughts\".\n",
        "\n",
        "- The **\"thee of thoughts\"** is constructed in the following way:\n",
        "  - If a \"thought\" starts with `Alternatively`, `Wait`, or `But wait`, we query using another LLM (**Llama-3.1-70B** by default) to determine which of the previous \"thoughts\" is continued by this. If it starts a completely different solution, the thought is connected to **root** (empty solution; the very start).\n",
        "  - Otherwise, the thought is connected with the previous one.\n",
        "\n",
        "- Finally, the tree is saved as `thought_analysis/thought_tree.png`, if you didn't change default output path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2yGX-XD47Av"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMu58cxChOMI"
      },
      "outputs": [],
      "source": [
        "!curl -o thought_analysis.py https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic2/thought_analysis.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwS4qdLbtB15"
      },
      "source": [
        "Let's also download a sample solution for us to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2GczUGFtFCb"
      },
      "outputs": [],
      "source": [
        "!curl -o sample_solution.txt https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic2/sample_solution.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "qt_JSdbUpn9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdZTfNwlhOO5"
      },
      "outputs": [],
      "source": [
        "from thought_analysis import analyze_solution_thoughts\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "hf_access_token = userdata.get('hf_access_token')\n",
        "os.environ[\"HF_TOKEN\"] = hf_access_token\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\")\n",
        ")\n",
        "model = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
        "\n",
        "reasoning_model = \"deepseek-ai/DeepSeek-R1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(reasoning_model,\n",
        "                                          hf_access_token=hf_access_token)\n",
        "\n",
        "file_path = \"sample_solution.txt\"  # Path to the solution file\n",
        "#file_path = \"deepseek_solutions/math41.txt\"  # requires downloading zip file below\n",
        "output_dir = \"thought_analysis\"  # Directory to save results\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    solution_text = file.read()\n",
        "\n",
        "\n",
        "connections, viz_path, summary_path = analyze_solution_thoughts(\n",
        "    solution_text,\n",
        "    client=client,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    output_dir=output_dir,\n",
        "    min_split_size=120\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('thought_analysis/thought_tree.png')"
      ],
      "metadata": {
        "id": "7NfS8zrlqwrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVzi-kxRwewD"
      },
      "source": [
        "We can look closely at the connection or just check the `.png` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZy8RQHFhOR9"
      },
      "outputs": [],
      "source": [
        "connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti9CFYW7E1dU"
      },
      "source": [
        "**Your task**. Take some other solutions, construct their trees of thoughts.\n",
        "\n",
        "You may also play with solutions generated by **DeepSeek R1**. We created several of them for you:\n",
        "\n",
        "- 10 from the MATH benchmark,\n",
        "- 10 from the AIME benchmark,\n",
        "- 2 from the Frontier Math benchmark.\n",
        "\n",
        "Just beware that these solutions will be much, much longer than the solutions by **QWQ**.\n",
        "\n",
        "You can download them, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C8tZJtNhOU_"
      },
      "outputs": [],
      "source": [
        "!gdown 1TpROB-8XAE6z1OlTfli7XB6YoY6WQi18\n",
        "!unzip deepseek_solutions.zip -d deepseek_solutions/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMVuGm5OFdKs"
      },
      "source": [
        "And, of course, feel free to create your own solutions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7flUkBQFlXb"
      },
      "source": [
        "### Size of Branches of Thoughts\n",
        "\n",
        "**This is a task for you**! Create a simplified fuction\n",
        "\n",
        "```\n",
        "thoughts, token_counts = decompose_solution_thoughts(sample_text,\n",
        "                                                     tokenizer=tokenizer)\n",
        "```\n",
        "\n",
        "that, given a solution `sample_text` and a tokenizer, returns:\n",
        "\n",
        "- `thoughts` which is a split of `sample_text` by exactly `Alternatively`, `Wait`, `But wait` (you may add some of their synonyms you'll spot in the solutions). So, we only keep track of the **whole branches of the \"tree of thoughts\"** here.\n",
        "- `token_counts` - the number of tokens in each of these \"branches\".\n",
        "\n",
        "Now, we suggest you to explore the size of these branches in tokens. Create a histogram of branch sizes. What can you say about its shape? Take a look at several extremely long branches - what do you think, why are they so long? Check the shortest branches. What happens there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE_ao9lNhOX3"
      },
      "outputs": [],
      "source": [
        "def decompose_solution_thoughts(sample_text: str, tokenizer=tokenizer):\n",
        "  def tokenize(text: str):\n",
        "    return tokenizer.encode(text)\n",
        "\n",
        "  def rsplit(text: str):\n",
        "    import re\n",
        "    return re.split(r'(Alternatively)|(Wait)|(But wait)', text, flags=re.IGNORECASE)\n",
        "\n",
        "  thoughts = rsplit(sample_text)\n",
        "\n",
        "  token_counts = list(map(lambda branch: len(tokenize(str(branch))), rsplit(sample_text)))\n",
        "  return (thoughts, token_counts)\n",
        "\n",
        "with open(\"deepseek_solutions/math41.txt\", 'r', encoding='utf-8') as file:\n",
        "  solution_text = file.read()\n",
        "  (thoughts, token_counts) = decompose_solution_thoughts(solution_text)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plt.hist(token_counts, density=True)\n",
        "\n",
        "  plt.xlabel('Value')\n",
        "  plt.ylabel('Frequency')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpIddZGIS80"
      },
      "source": [
        "### LLM underthinking\n",
        "\n",
        "This part is inspired by [Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs](https://arxiv.org/pdf/2501.18585). This paper investigated connection between the length of thought branches (fragments of solution between \"Alternatively\", \"Wait\", etc) and the solution accuracy. The found out that in many cases LLMs abandon promising solutions, cutting thought branches short before they could come to fruition - and this might contribute to failure of the whole solution.\n",
        "\n",
        "**Your task**: create on one plot two histograms of branch lengths - one histogram for tasks with correct answer and one of tasks with incorrect answer. You can find information about answer correctness in `qwq_results[\"evaluation_log\"]` (`\"is_correct\"` fields).\n",
        "\n",
        "Since there is a different number of correct and incorrect answers in the data, we recommend normalizing the histograms so that they show frequency instead of count. This may be done by setting `density=True`. Do you see any specific patterns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3xpa5LDKJTj"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "def extract_data(log):\n",
        "  (thoughts, token_counts) = decompose_solution_thoughts(log['model_response'])\n",
        "  return (token_counts, log['is_correct'])\n",
        "\n",
        "results = [extract_data(p) for p in qwq_results['evaluation_log']]\n",
        "correct = [p[0] for p in results if p[1] == True]\n",
        "incorrect = [p[0] for p in results if p[1] == False]\n",
        "\n",
        "correct_sizes = reduce(lambda a, b: a + b, correct)\n",
        "incorrect_sizes = reduce(lambda a, b: a + b, incorrect)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "correct_plt = fig.add_subplot(2, 1, 1)\n",
        "correct_plt.hist(correct_sizes, density=True)\n",
        "correct_plt.title.set_text(\"Correct\")\n",
        "\n",
        "incorrect_plt = fig.add_subplot(2, 1, 2, sharex=correct_plt)\n",
        "incorrect_plt.hist(incorrect_sizes, density=True)\n",
        "incorrect_plt.title.set_text(\"Incorrect\")\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOk_pbsVKMHZ"
      },
      "source": [
        "## Task 2. Convince me with smiles\n",
        "\n",
        "This is a continuation of the \"reasoning dot by dot\" section.\n",
        "\n",
        "**Your task**: still working with **Llama-3.1-8B**, try to prompt it to output other things instead of reasoning. We personally recommend trying dots, commas, pluses, and smiles - they'll produce diverse and interesting patterns. However, feel free to try whatever you like! Don't forget to print several examples of this \"reasoning\". They might give you some additional insights.\n",
        "\n",
        "Also, try running the same experiment with **Qwen/Qwen2.5-32B-Instruct**. Will you see a similar effect? Try changing not only models, but also prompts. Check how if affects the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntwrjf1LI9eb"
      },
      "outputs": [],
      "source": [
        "# <YOUR EXPERIMENTS HERE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USMGD8D6aUJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}