{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Silvmike/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "# 2.1. Structured Inputs and Outputs\n",
        "\n",
        "In Topic 1, we learnt how to prompt an LLM in such a way that it understands what you want from it and gives a relevant answer. In this notebook we'll continue this discussion by understanding\n",
        "\n",
        "* How to make prompts reusable by using **prompt templates**\n",
        "* How to ensure that an LLM creates its outputs in a convenient, easily parsable format\n",
        "\n",
        "Let's start by running some code which will help us in the whole notebook:"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB",
        "outputId": "0994b922-ccae-4ea7-ce16-33916103da9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/683.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/683.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m675.8/683.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE:\n",
        ">\n",
        ">![Note on secrets](https://drive.usercontent.google.com/u/0/uc?id=1i13yXeTgb9vnsBkLC1goA7MM4QzfJLsk&export)"
      ],
      "metadata": {
        "id": "Dpe7nK4ioSz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "L3wUdp21e0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an LLM-powered system, there's always a layer of prompting logic hidden from the user. For example, ChatGPT, Claude, Gemini and others have quite elaborate **system prompts** that set up rules and guardrails of LLM's communication with the user.\n",
        "\n",
        "However, in some cases a system prompting isn't a flexible enough mechanism. Imagine, for example,\n",
        "\n",
        "* a customer support bot that needs to be aware of the user's geography to give relevant answers about locally available products\n",
        "* a railway service support bot that needs to be aware of today's railway strikes and other calamities\n",
        "\n",
        "You'll likely need to insert this information in the middle of the prompt; and for such things, **prompt templates** are a great tool."
      ],
      "metadata": {
        "id": "xeHMXwGEe0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a **prompt template** is a template string like\n",
        "\n",
        "```python\n",
        "\"some fixed information {template placeholder 1}\n",
        "some more fixed information {template placeholder 2}\"\n",
        "```\n",
        "\n",
        "where the template placeholders are to be filled in just before an actual LLM call.\n",
        "\n",
        "Let's check several neat ways of wrapping this logic.\n",
        "\n",
        "First of all, you can write your own wrapper. In the example below, `m['content'].format(**kwargs)` allows to put as much formatting as you wish into the user's message."
      ],
      "metadata": {
        "id": "6i-MRNQMZvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class MessagesPromptTemplate():\n",
        "    messages: List[Dict]\n",
        "\n",
        "    def __init__(self, messages: List[Dict]):\n",
        "        self.messages = messages\n",
        "\n",
        "    def format(self, **kwargs):\n",
        "        return [\n",
        "            {\n",
        "                \"role\":  m['role'],\n",
        "                \"content\": m['content'].format(**kwargs)\n",
        "            }\n",
        "            for m in self.messages\n",
        "        ]"
      ],
      "metadata": {
        "id": "ZCv8sZ5Be0DC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = MessagesPromptTemplate(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You only answer in rhymes\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about {city}\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wrNIeKu3e0DD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(city=\"Paris\")"
      ],
      "metadata": {
        "id": "kjL-E-bGe0DD",
        "outputId": "075c3bff-56f0-4e5f-e894-b14229f0e7bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Paris'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try calling an llm with different variables"
      ],
      "metadata": {
        "id": "jspgVhL0e0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Paris\"),\n",
        "    model=llama_model\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "DuR5IqTGe0DD",
        "outputId": "1ec963e0-e9b0-4e58-ec22-040a5772a054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh so fine, I'll tell you in time,\n",
            "About Paris, the city that's truly sublime.\n",
            "It's the land of love, of art and of light,\n",
            "The Eiffel Tower shines with pure delight.\n",
            "\n",
            "The Seine River flows, a gentle stream,\n",
            "Through the heart of the city, a wondrous dream.\n",
            "The Louvre's grand halls, a treasure to see,\n",
            "The Mona Lisa smiles, a mystery to me.\n",
            "\n",
            "Montmartre's hills, a bohemian sight,\n",
            "Street artists and cafes, a joyful delight.\n",
            "The Champs-Élysées, a famous boulevard so wide,\n",
            "A shopper's paradise, where fashion does reside.\n",
            "\n",
            "So come to Paris, and let your heart sing,\n",
            "This city of romance, will make your spirit take wing.\n",
            "It's a place of enchantment, a city of dreams,\n",
            "Paris, the capital, of France, it seems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Amsterdam\"),\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "_qP1h3ahe0DE",
        "outputId": "b2a2f6cd-52ee-4dc8-f5e3-0fb038421b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Amsterdam, the city so fine,\n",
            "The canals shine, and the culture divine.\n",
            "The Rijksmuseum's art, a treasure to see,\n",
            "And the Anne Frank House, a history to be.\n",
            "\n",
            "The Vondelpark's green, a peaceful place to stroll,\n",
            "And the Jordaan's charm, with its shops and its roll.\n",
            "The food's delicious, with cheeses so bright,\n",
            "And the nightlife's lively, with a party in sight.\n",
            "\n",
            "The bicycles abound, with a ring and a bell,\n",
            "And the city's friendly, with a story to tell.\n",
            "So come to Amsterdam, and take in the view,\n",
            "You'll fall in love, with this city anew!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template class we've written is very primitive and would fail if, for example, some keys aren't inputted.\n",
        "\n",
        "One of the good implementations of prompt templates can be found in LangChain [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/)"
      ],
      "metadata": {
        "id": "3nSqLBC2e0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU"
      ],
      "metadata": {
        "id": "m6nkay_qe0DE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You only answer in rhymes\"),\n",
        "    (\"user\", \"Tell me about {city}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"city\": \"Madrid\"})"
      ],
      "metadata": {
        "id": "GeZEj5pCe0DE",
        "outputId": "6a288673-612e-4ce8-bf50-6e20c68b5c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You only answer in rhymes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Madrid', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You don't have to use LangChain llm calls or anything else, you can only take their PromptTemplate implementation.\n",
        "\n",
        "However, there's quiet a bit of useful code in that library."
      ],
      "metadata": {
        "id": "5zweywyae0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages"
      ],
      "metadata": {
        "id": "8SyzAiTge0DF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated_messages = convert_to_openai_messages(prompt_template.invoke({\"city\": \"Madrid\"}).to_messages())\n",
        "templated_messages"
      ],
      "metadata": {
        "id": "xvE2qTQce0DF",
        "outputId": "474d3404-2663-4b8d-eca5-29c8788b39e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Madrid'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=templated_messages,\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "awT49JQve0DF",
        "outputId": "08acddfa-8feb-43f4-beb8-3e3fa7445ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Madrid, the city's so fair,\n",
            "A destination beyond compare.\n",
            "The capital of Spain, don't you know,\n",
            "A place where culture and fun will grow.\n",
            "\n",
            "The Prado Museum's a must-see sight,\n",
            "With art and history, shining so bright.\n",
            "The Royal Palace, a grandeur to see,\n",
            "A symbol of power, for you and me.\n",
            "\n",
            "Tapas and paella, a culinary delight,\n",
            "In the evenings, the streets come alive with light.\n",
            "Flamenco music and dance, a passionate treat,\n",
            "In Madrid, the energy's always on the street.\n",
            "\n",
            "So if you ever visit, don't be shy,\n",
            "Explore the city, and let your spirit fly.\n",
            "Madrid's charm will captivate your heart,\n",
            "And you'll leave with memories, a brand new start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring LLM outputs\n",
        "\n",
        "In many cases you require not just a free text answer, but something particular you can use later in your system. For example, if you want your LLM to classify a customet's intent to later pass the conversation to a relevant department, you need to extract the particular intent class from the LLM's answer.\n",
        "\n",
        "To parse your LLM outputs conveniently, it's wise to structure them in a specific way. We've already discussed some prompting tricks in Topic 1; this time, we'll learn several more reliable ways of making the LLM abide a designated output format."
      ],
      "metadata": {
        "id": "PLtqUw9ce0DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic output structuring\n",
        "\n",
        "As a basic way to structure your output, you can \"ask\" an LLM to present the output in a specific format. For example:"
      ],
      "metadata": {
        "id": "Wb8TpmGZe0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Design one role play character\\'s name, class and a short description.\n",
        "Present it as a markdown list\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "liGqj3PKe0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is quite good, it's not very reliable. A better way would be to show some examples to LLM so that it knows what we expect.\n",
        "\n",
        "These examples are known as **few-shot examples** and the prompting technique itself - as **few-shot prompting**."
      ],
      "metadata": {
        "id": "y7U9nI3Ae0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Design one role play character\\'s name, class and a short description. Present it as a markdown list.\\n'\\\n",
        "            \"Examples:\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Randalf the Yellow;\\n\"\\\n",
        "            \"- **Class:** Fire mage;\\n\"\\\n",
        "            \"- **Proficiency:** Pyro magic;\\n\"\\\n",
        "            \"- **Resistance:** Fire;\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Bonan;\\n\"\\\n",
        "            \"- **Class:** Barbarian;\\n\"\\\n",
        "            \"- **Proficiency:** Axe;\\n\"\\\n",
        "            \"- **Resistance:** Mental magic;\\n\"\\\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "I5lonLame0DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, LLM captured the format pretty well.\n"
      ],
      "metadata": {
        "id": "PzygVBe0e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Solve the following equation and output only the answer number without reasoning after \"Answer:\"\\n' \\\n",
        "            '123 * 321 = ?\\n' \\\n",
        "            'Answer:'\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "S4KXsxtZe0DG",
        "outputId": "07383fe7-7e02-4c81-da9f-019104f0d26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the answer isn't correct (LLMs are notoriously bad at arithmetics), the output structure is correct and easy to parse out."
      ],
      "metadata": {
        "id": "BtVmakLFe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can do even better."
      ],
      "metadata": {
        "id": "JNQ0ns6Oe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "_2G1O-w6e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = nebius_client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=llama_model,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "json_output"
      ],
      "metadata": {
        "id": "Q88QagE-e0DG",
        "outputId": "ed178b7e-43dc-4c9d-a85f-44647ee69c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \\n\"class\": \"Shadow Assassin\", \\n\"description\": \"A stealthy and deadly assassin from the underworld, with unparalleled agility and cunning.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "m4KVrmlFe0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "b9-7ouAie0DH",
        "outputId": "28064470-6e07-48b8-bad2-33c47a90dede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Eryndor Thorne',\n",
              " 'class': 'Shadow Assassin',\n",
              " 'description': 'A stealthy and deadly assassin from the underworld, with unparalleled agility and cunning.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model to create a schema for our outputs:"
      ],
      "metadata": {
        "id": "tTrOKdoee0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = nebius_client.chat.completions.create(\n",
        "    model=llama_model,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": CharacterProfile.model_json_schema()\n",
        "    }\n",
        ")\n",
        "\n",
        "CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dki1ECRee0DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dd58cb-bf86-414f-917b-90785e6b5f84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Eira Shadowglow', age=25, special_skills=['Stealth', 'Archery', 'Survival'], traits=['Independent', 'Resourceful', 'Mysterious'], character_class='Ranger', origin=\"Eira was born in a small village on the edge of a vast and mysterious forest. She grew up learning the ways of the wilderness from her ranger father, who disappeared on a mission when she was a teenager. Eira has since dedicated her life to tracking down her father's disappearance and protecting the innocent from the shadows.\")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "2viyoMVOe0DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to structure outputs is using examples\n",
        "\n",
        "Let's consider an example from a famous [MMLU dataset](https://huggingface.co/datasets/cais/mmlu):"
      ],
      "metadata": {
        "id": "TpeDDvNse0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which of the following statements about Ethernets is typically FALSE?\"\n",
        "\n",
        "A = \"Ethernets use circuit switching to send messages.\"\n",
        "B = \"Ethernets use buses with multiple masters.\"\n",
        "C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\"\n",
        "D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\"\n",
        "\n",
        "correct_answer = \"A\""
      ],
      "metadata": {
        "id": "tygKuanMe0DI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we want our LLM to solve this \"test\" by answering to us with a letter corresponding to the right answer. This will also make calculating metrics much easier. Let's see what would happen."
      ],
      "metadata": {
        "id": "s5vvF33Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cG2GCODVe0DI",
        "outputId": "b5f211ae-f752-4fb7-faf1-e84501a7fd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Ethernets use circuit switching to send messages.\n",
            "\n",
            "Ethernets actually use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single connection for the duration of the transmission. \n",
            "\n",
            "The other options are generally true: \n",
            "- B: Ethernets do use buses (or more commonly now, switches) with multiple masters, allowing multiple devices to initiate transmissions.\n",
            "- C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), use a collision-detection method to manage how devices access the network and handle collisions when two devices try to transmit at the same time.\n",
            "- D: It's true that networks connected by traditional Ethernet cables are limited in length (typically up to 100 meters for standard Ethernet cables) due to signal degradation over distance, though this can be extended with repeaters or switches. \n",
            "\n",
            "Thus, the statement about Ethernets using circuit switching is the one that is typically false.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it did output the right answer, but if we do a simple comparison, we'll get into trouble:"
      ],
      "metadata": {
        "id": "TaJNBEK7e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "hIadZJfVe0DI",
        "outputId": "69b0dfee-3cf7-467a-ea10-a83ba2176823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's teach our model to answer in the right way using so-called Few Shot Prompting also known as In-Context Learning. We essentially show the model some examples in the prompt to teach it in which format we want the answer to be"
      ],
      "metadata": {
        "id": "v6igGq25e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Examples:\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Task:\n",
        "Answer the following question with one of the options listed below. Only ouput the answer in the same format as the examples.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ahOoSsxhe0DI",
        "outputId": "3ac699cf-a654-43a5-de3c-021f5de1ffc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "_wlgxkEue0DI",
        "outputId": "211f146d-25b1-437b-897c-14c83a60ea60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have observed that for some models the dialog format is actually a better way to structure the Few-Shot examples"
      ],
      "metadata": {
        "id": "uwEVq78Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "Assitant:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WsQW4lNqe0DJ",
        "outputId": "48679036-30e6-4cb5-bb87-918c63b4c6d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Explanation: Ethernets actually use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, whereas circuit switching dedicates a channel to a single connection for the duration of the transmission. \n",
            "\n",
            "So, the correct answer is: A: Ethernets use circuit switching to send messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically we don't even need to show the model relevant examples if we want it to learn the output formatting"
      ],
      "metadata": {
        "id": "rClIT520e0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Question: Choose the letter A\n",
        "A: A\n",
        "B: B\n",
        "C: C\n",
        "D: D\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which is the biggest number?\n",
        "A: 1\n",
        "B: 2\n",
        "C: 3\n",
        "D: 4\n",
        "Answer:\n",
        "D\n",
        "\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bUyAniI1e0DJ",
        "outputId": "8b5f20d3-1ab3-4c52-a9f0-6444a69d83cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Explanation: Ethernets typically use packet switching, not circuit switching. Circuit switching is a method where a dedicated communication path is established between two nodes before data is sent, whereas packet switching allows multiple devices to share the same communication path. \n",
            "\n",
            "The other options are true: Ethernets do use buses with multiple masters (option B), Ethernet protocols use a collision-detection method (CSMA/CD) to manage the sharing of the communication medium (option C), and networks connected by Ethernets are generally limited in length to a few hundred meters due to signal degradation (option D). \n",
            "\n",
            "Therefore, option A is the statement that is typically FALSE about Ethernets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes you can confuse the model if you have examples from the distribution, which is different than your data's one. So for the best results try to match the distribution."
      ],
      "metadata": {
        "id": "dWhLwogce0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "sH-D2q9te0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python -qU"
      ],
      "metadata": {
        "id": "ML_0cLRre0DJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Tavily API key which you can get from [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "Then either use google's secret storage or put it into a file and upload it."
      ],
      "metadata": {
        "id": "g9r9qF5Qe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ['TAVILITY_API_KEY\"] = open(\".tavily_api_key\").read()\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "response = tavily_client.search(\"Who is Leo Messi?\", topic=\"general\")\n",
        "\n",
        "print(response['results'])"
      ],
      "metadata": {
        "id": "RjdV-AHye0DJ",
        "outputId": "0ebb0bbd-d209-45c6-c248-1cbd8b8f1726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'title': 'Lionel Messi - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Lionel_Messi', 'content': \"Widely regarded as one of the greatest players of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and eight times being named the world's best player by FIFA.[note 2] He is the most decorated player in the history of professional football having won 45 team trophies,[note 3] including twelve Big Five league titles, four UEFA Champions Leagues, two Copa Américas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists (192) in La Liga, most assists (18) and goal contributions (32) in the Copa América, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright.\", 'score': 0.80698997, 'raw_content': None}, {'title': 'Lionel Messi: Biography, Soccer Player, Inter Miami CF, Athlete', 'url': 'https://www.biography.com/athletes/lionel-messi', 'content': 'Lionel Messi, a forward for Inter Miami CF, is one of the world’s greatest soccer players and helped the Argentina national team win its third FIFA World Cup in 2022. Messi, now playing for Inter Miami CF of the MLS, helped his home country win soccer’s biggest event for the first time since 1986, scoring two goals in the final and leading Argentina to a 4-2 win over Kylian Mbappé and France on penalties. Lionel Messi is an Argentinian soccer player who has played for FC Barcelona, Paris Saint-Germain, and currently, the MLS club Inter Miami CF as well as the Argentina national team.', 'score': 0.79901963, 'raw_content': None}, {'title': 'Official site', 'url': 'https://messi.com', 'content': 'Web oficial Leo Messi jugador del Inter de Miami – messi.com – Web oficial de Lionel Messi, jugador del Futbol Profesional y campeón mundial con la selección Argentina Web oficial Leo Messi jugador del Inter de Miami - messi.com Inter Miami CF disputa este domingo la novena jornada de la MLS ante Dallas en el Chase Stadium. Leo se enfrenta este sábado a Columbus Crew en la octava jornada de la MLS para Inter Miami en el […] Inter Miami y Chicago Fire empataron 0-0 en el partido correspondiente a la jornada 7 de la MLS del encuentro […] Leo Messi Management S.L.U. utiliza cookies propias y de terceros para ofrecerle contenidos adaptados a sus intereses.', 'score': 0.69622743, 'raw_content': None}, {'title': 'Lionel Messi Biography - Facts, Childhood, Family Life & Achievements', 'url': 'https://www.thefamouspeople.com/profiles/lionel-messi-5242.php', 'content': \"Lionel Messi has won multiple FIFA Ballon d'Or awards, numerous La Liga titles with FC Barcelona, and holds the record for most goals scored in a calendar year. In the finals too Messi scored the winning goal to give Barcelona their third title in six years and fourth overall. In the 2018 Football World Cup Messi scored a goal in the Argentina's final group match against Nigeria and helped his team to a 2-1 victory. Messi has in his kitty 20 Player of the Year awards including FIFA World Player of the Year (1), World Soccer Player of the Year (3), Goal.com Player of the Year (2), UEFA Best Player in Europe Award (1), UEFA Club Footballer of the Year (1), FIFA U-20 World Cup Player of the Tournament (1), La Liga Player of the Year (3), La Liga Foreign Player of the Year (3) and La Liga Ibero-American Player of the Year (5).\", 'score': 0.5792344, 'raw_content': None}, {'title': \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\", 'url': 'https://www.britannica.com/biography/Lionel-Messi', 'content': 'Lionel Messi scored 73 goals during the 2011–12 season while playing for FC Barcelona, breaking\\xa0a 39-year-old record for single-season goals in a major European football league. Britannica Quiz Great Moments in Sports QuizMessi’s play continued to rapidly improve over the years, and by 2008 he was one of the most dominant players in the world, finishing second to Manchester United’s Cristiano Ronaldo in the voting for the 2008 Ballon d’Or. In early 2009 Messi capped off a spectacular 2008–09 season by helping FC Barcelona capture the club’s first “treble” (winning three major European club titles in one season): the team won the La Liga championship, the Copa del Rey (Spain’s major domestic cup), and the Champions League title.', 'score': 0.54782754, 'raw_content': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `query` and `topic` parameters.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameters are for. Note that it's not for you, but for the LLM :) So please make sure you provide a clear explanation.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "IWspcVJxe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web-search\",\n",
        "            \"description\": \"Retrieves results from web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Search topic either 'general' or 'news'\",\n",
        "                        \"enum\": [\"general\", \"news\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "Y00ckl3Ie0DJ",
        "outputId": "e9266465-b644-46d5-da93-4edeec86d84e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-49599bbaa4e349a18ededee5d1c38fcb', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f1e3bca504442cb0f817cb437ff7fd', function=Function(arguments='{\"query\": \"Shrek cat name\", \"topic\": \"general\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1747576653, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=267, total_tokens=299, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also try to ask for some news-worthy content to see if LLM decides on a different `topic`."
      ],
      "metadata": {
        "id": "xcU7jtcoe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What happened in London today?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "gx4QpV3Le0DK",
        "outputId": "5f5a0a4f-8aea-42ba-9b8e-0735d381a764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-0449cbbd238d4ba59c9f14bb6285c114', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-017533fd738f42e6b02167176b11d1e3', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1747576658, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=262, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "n94smyzfe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "id": "-AMA-dM9e0DK",
        "outputId": "193a65aa-b503-4bcb-aa7c-39ae6e76747d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessageToolCall(id='chatcmpl-tool-017533fd738f42e6b02167176b11d1e3', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering, why do we include tool usage in structured output topic.\n",
        "\n",
        "Thing is, you can also use this functionality to structure your output. You don't have to use a real function as your tool. Let's use our previous example"
      ],
      "metadata": {
        "id": "hDqCaytPe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_rpg_character\",\n",
        "            \"description\": \"Creates a character based on attributes and description\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the character\",\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Age of the character\",\n",
        "                    },\n",
        "                    \"special_skills\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of special skills of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"traits\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of traits of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"character_class\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Class of the character\",\n",
        "                        \"enum\": [\"mage\", \"rogue\", \"barbarian\", \"knight\", \"paladin\"]\n",
        "                    },\n",
        "                    \"origin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Origin of the character\",\n",
        "                        \"enum\": [\"human\", \"elf\", \"orc\", \"undead\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"age\", \"special_skills\", \"traits\", \"character_class\", \"origin\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATabJqL4e0DK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked to create a character, use `create_rpg_character` tool.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Generate a random character for my new session\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response.choices[0].message.tool_calls[0].function.arguments"
      ],
      "metadata": {
        "id": "uFf2dqmPe0DK",
        "outputId": "c9227b12-4871-4f78-b743-156d245dbd81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"age\": 25, \"special_skills\": [\"stealth\", \"magic\"], \"traits\": [\"courageous\", \"charismatic\"], \"character_class\": \"mage\", \"origin\": \"human\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice tasks**\n",
        "\n",
        "If you encounter any difficulties or simply want to see our solutions, feel free to check the [Solutions notebook](https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs_solutions.ipynb)."
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. LLM Information extraction\n",
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes your \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"store_to_db\",\n",
        "            \"description\": \"Puts short information on event to the database\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the event\",\n",
        "                    },\n",
        "                    \"date\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Dates of the event in dd.MM.yyyy[-dd.MM.yyyy] format, example-1 (if event starts and ends on the same day - there must be only signle date): 05.11.2023, example-2 (event duration is more than one day, so it must be a range): 05.12.2023-15.12.2023\",\n",
        "                    },\n",
        "                    \"n_participants\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Amount of participants, example-1: 340. Omitted when participants were not mentioned.\",\n",
        "                    },\n",
        "                    \"n_speakers\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Amount of speakers (people giving a public speech or presenting something to other people) of the event. Omitted when speakers were not mentioned.\",\n",
        "                    },\n",
        "                    \"price\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Price in US dollars, example-1: USD 250, example-2: EUR 179, example-3: None (when price is not mentioned)\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"name\", \"date\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "W7rnlb-BthJ7"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_press_release_prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"Your goal is to extract short information on the event from the given text, and store it in the database. A text might contain a date of its writing, and it might be different to the date of the event. Text:\"),\n",
        "    (\"user\", \"{text}\")\n",
        "])\n",
        "\n",
        "def parse_press_release(pr: str) -> dict:\n",
        "    messages = convert_to_openai_messages(parse_press_release_prompt_template.invoke({\"text\": pr}).to_messages())\n",
        "    chat_response = nebius_client.chat.completions.create(\n",
        "        messages=messages, tools=tools, model=llama_model, temperature=0.3\n",
        "    )\n",
        "    return json.loads(chat_response.choices[0].message.tool_calls[0].function.arguments)"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "id": "dTVTe284e0DL",
        "outputId": "70c30446-2d2a-4e5d-e254-fc6dfe3b9e2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv\n",
            "To: /content/press_release_extraction.csv\n",
            "16.0kB [00:00, 6.14MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "OJjcT2pSe0DL",
        "outputId": "76af2f44-9ba1-44d3-db2a-0e6de88c24da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             pr_text  \\\n",
              "0  InnovAI Summit 2023: A Glimpse into the Future...   \n",
              "1  Press Dispatch: 'Artificial Mariners: Navigati...   \n",
              "2  FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...   \n",
              "3  Press Release: Cutting-Edge Innovations Debute...   \n",
              "4  Press Release: Innovative Minds Gather at \"AI ...   \n",
              "\n",
              "                                           pr_parsed  \n",
              "0  {\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...  \n",
              "1  {\"name\": \"Artificial Mariners: Navigatin' the ...  \n",
              "2  {\"name\": \"Annual Machine Learning Symposium 20...  \n",
              "3  {\"name\": \"AI Advancements Summit 2023\",\\n \"dat...  \n",
              "4  {\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae597f6e-c6af-437a-93dc-47310ac9afb4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_text</th>\n",
              "      <th>pr_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>InnovAI Summit 2023: A Glimpse into the Future...</td>\n",
              "      <td>{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Press Dispatch: 'Artificial Mariners: Navigati...</td>\n",
              "      <td>{\"name\": \"Artificial Mariners: Navigatin' the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...</td>\n",
              "      <td>{\"name\": \"Annual Machine Learning Symposium 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Press Release: Cutting-Edge Innovations Debute...</td>\n",
              "      <td>{\"name\": \"AI Advancements Summit 2023\",\\n \"dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Press Release: Innovative Minds Gather at \"AI ...</td>\n",
              "      <td>{\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae597f6e-c6af-437a-93dc-47310ac9afb4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae597f6e-c6af-437a-93dc-47310ac9afb4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae597f6e-c6af-437a-93dc-47310ac9afb4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9f3ddbe8-a5e9-4c9f-bfad-d1d1d2e2e585\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f3ddbe8-a5e9-4c9f-bfad-d1d1d2e2e585')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9f3ddbe8-a5e9-4c9f-bfad-d1d1d2e2e585 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pr_df",
              "summary": "{\n  \"name\": \"pr_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"pr_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\\n\\nCity of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\\n\\nEsteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\\n\\nThis year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\\n\\nAmong other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\\n\\nThe event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\\n\\nFor media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\",\n          \"Press Dispatch: 'Artificial Mariners: Navigatin' the AI Seas' - The Grand AI and Machine Learnin' Symposium of 2023\\n\\nOctober 12, 2023 - Tortuga Bay, The Spanish Main - Avast ye! Just a fortnight past, the shores of Tortuga Bay were graced with the grandest gatherin' of minds and marauders from across the seven seas. The event known far and wide as \\\"Artificial Mariners: Navigatin' the AI Seas\\\" did cast its anchor on the 8th and 9th of October, bringin' together a motley crew of over 2,000 sea dogs, scholars, and ship captains keen on decipherin' the mysteries of Artificial Intelligence and Machine Learnin'.\\n\\nWith the Jolly Roger flyin' high, the symposium boasted of tales and tools shared by the likes of the fearsome Dr. \\\"Blackwater\\\" Aria Kessler, known in the New World and Old for her dark arts in makin' machines mimic the mind. There were whispers amongst the crew about Dr. Jun \\\"Kraken\\\" Zhao, who spoke of harnessin' the monstrous power of calculations with nary a need for extra rum, savin' energy like a true sea scoundrel.\\n\\nNo gatherin' of this sort would be complete without explorin' the depths of ethical plunderin', led by the sharp-witted Dr. Sofia \\\"Siren\\\" \\u00c1lvarez. Her talk drew maps for navigatin' the fine line 'twixt progress and plunder, makin' sure the power of our newfound intelligence be used for the good of all, not just the few.\\n\\nBut shiver me timbers, it weren\\u2019t all just jabber! Tom\\u00e1s \\\"One-Eye\\\" Rivera, a cunning pirate with a penchant for codes and ciphers, put together a hands-on spectacle. This live code-crackin' session saw shipmates and buccaneers alike put their heads together, tacklin' problems that'd make even the saltiest of sea dogs sweat.\\n\\nAs the sun set on the final day, the renowned and somewhat mystical Dr. Emilia \\\"Seafarer\\\" van der Meer took to the stage, her eyes alight with visions of uncharted waters. Her words wove tales of futures where our trusty shipmates, the AI, be integral to weatherin' the storms ahead, safeguardin' not just our gold, but our lands and livelihoods.\\n\\n\\\"'Twas a rally like no other, fillin' our hearts with fire and our minds with dreams of treasures that lay beyond the horizons of man and machine,\\\" confessed an old tar as he prepared to disembark.\\n\\nAs the symposium closed its doors, the air was thick with plans and plots, the promise of alliances, and a shared quest to conquer the vast, unpredictable seas of Artificial Intelligence.\\n\\nFor those wishin' to re-live the adventure or who couldn\\u2019t sail with us this time around, visit [Symposium\\u2019s Mysterious Cove].\\n\\nContact for parley:\\nName: [Your trusty informant]\\nTitle: [Harbormaster of Information]\\nBird-mail: [Email]\\nMessage in a bottle: [Phone Number]\",\n          \"Press Release: \\\"AI for Equity Summit\\\" Champions Inclusivity in Technology\\n\\nOctober 18, 2023 \\u2014 The transformative \\\"AI for Equity Summit\\\" convened on October 15, 2023, marking an historic congregation of technological prowess dedicated to inclusivity and equitable advancements in artificial intelligence. The event welcomed over 3,000 attendees, each contributing a registration fee of $250, signifying their commitment to nurturing diversity in AI.\\n\\nSix illustrious speakers graced the summit's virtual stage, including Dr. Ayesha Khurram, renowned for her work in ethical AI, tech visionary Omar Svensson, Dr. Lola Adebayo, an advocate for minorities in STEM, Dr. Ji-hoon Park, known for his innovative algorithms against biases, Maria Navarro, a crusader for women in tech, and coding prodigy, Zane Kirschner, who is making significant strides in accessible AI for persons with disabilities.\\n\\nThe summit wasn't just about discussions; it was about making tangible strides. The highlight was the launch of the \\\"AI Equity Initiative,\\\" a fund that collected over $1.2 million from participant contributions, aimed at supporting tech education in underserved regions.\\n\\nWorkshops emphasized actionable strategies for dismantling systemic barriers in the tech industry. Dr. Adebayo's session on 'Intersectionality in AI Development' and Kirschner's workshop titled 'Coding Without Barriers' particularly stood out for their depth and engagement.\\n\\nMoreover, the \\\"AI for Equity Summit\\\" was proud to allocate 20% of all registration fees toward scholarships for students from underrepresented backgrounds to pursue AI studies.\\n\\nIn the wake of the event, a communiqu\\u00e9 was released, expressing a unified pledge among the participants and speakers to proactively include underrepresented groups in both the development of AI and the conversations around it.\\n\\nAttendees and interested parties are encouraged to stay informed through the summit's official channels for ongoing initiatives and future events.\\n\\nEnd of Release\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr_parsed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"{\\n  \\\"name\\\": \\\"InnovAI Summit 2023\\\",\\n  \\\"date\\\": \\\"05.11.2023\\\",\\n  \\\"n_participants\\\": 3500,\\n  \\\"n_speakers\\\": 4,\\n  \\\"price\\\": \\\"None\\\"\\n}\",\n          \"{\\\"name\\\": \\\"Artificial Mariners: Navigatin' the AI Seas\\\",\\n \\\"date\\\": \\\"08.10.2023-09.10.2023\\\",\\n \\\"n_participants\\\": 2000,\\n \\\"n_speakers\\\": 5,\\n \\\"price\\\":\\\"None\\\"}\",\n          \"{\\\"name\\\": \\\"AI for Equity Summit\\\",\\n \\\"date\\\": \\\"15.10.2023\\\",\\n \\\"n_participants\\\":  3000,\\n \\\"n_speakers\\\": 6,\\n \\\"price\\\": \\\"USD 250\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "id": "sDGJ1vKxe0DL",
        "outputId": "87c9dc64-43b3-444f-f608-46ea0213c959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\": \"05.11.2023\",\\n  \"n_participants\": 3500,\\n  \"n_speakers\": 4,\\n  \"price\": \"None\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    print(row.pr_text)\n",
        "    print(\"\\n=================\")\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")"
      ],
      "metadata": {
        "id": "RSvVEFCWe0DL",
        "outputId": "60af56ae-9433-48d3-eb27-874cecec507f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
            "\n",
            "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
            "\n",
            "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
            "\n",
            "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
            "\n",
            "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
            "\n",
            "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
            "\n",
            "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\n",
            "\n",
            "=================\n",
            "For InnovAI Summit 2023 n_speakers 3 doesn't seem the same as 4\n",
            "Press Dispatch: 'Artificial Mariners: Navigatin' the AI Seas' - The Grand AI and Machine Learnin' Symposium of 2023\n",
            "\n",
            "October 12, 2023 - Tortuga Bay, The Spanish Main - Avast ye! Just a fortnight past, the shores of Tortuga Bay were graced with the grandest gatherin' of minds and marauders from across the seven seas. The event known far and wide as \"Artificial Mariners: Navigatin' the AI Seas\" did cast its anchor on the 8th and 9th of October, bringin' together a motley crew of over 2,000 sea dogs, scholars, and ship captains keen on decipherin' the mysteries of Artificial Intelligence and Machine Learnin'.\n",
            "\n",
            "With the Jolly Roger flyin' high, the symposium boasted of tales and tools shared by the likes of the fearsome Dr. \"Blackwater\" Aria Kessler, known in the New World and Old for her dark arts in makin' machines mimic the mind. There were whispers amongst the crew about Dr. Jun \"Kraken\" Zhao, who spoke of harnessin' the monstrous power of calculations with nary a need for extra rum, savin' energy like a true sea scoundrel.\n",
            "\n",
            "No gatherin' of this sort would be complete without explorin' the depths of ethical plunderin', led by the sharp-witted Dr. Sofia \"Siren\" Álvarez. Her talk drew maps for navigatin' the fine line 'twixt progress and plunder, makin' sure the power of our newfound intelligence be used for the good of all, not just the few.\n",
            "\n",
            "But shiver me timbers, it weren’t all just jabber! Tomás \"One-Eye\" Rivera, a cunning pirate with a penchant for codes and ciphers, put together a hands-on spectacle. This live code-crackin' session saw shipmates and buccaneers alike put their heads together, tacklin' problems that'd make even the saltiest of sea dogs sweat.\n",
            "\n",
            "As the sun set on the final day, the renowned and somewhat mystical Dr. Emilia \"Seafarer\" van der Meer took to the stage, her eyes alight with visions of uncharted waters. Her words wove tales of futures where our trusty shipmates, the AI, be integral to weatherin' the storms ahead, safeguardin' not just our gold, but our lands and livelihoods.\n",
            "\n",
            "\"'Twas a rally like no other, fillin' our hearts with fire and our minds with dreams of treasures that lay beyond the horizons of man and machine,\" confessed an old tar as he prepared to disembark.\n",
            "\n",
            "As the symposium closed its doors, the air was thick with plans and plots, the promise of alliances, and a shared quest to conquer the vast, unpredictable seas of Artificial Intelligence.\n",
            "\n",
            "For those wishin' to re-live the adventure or who couldn’t sail with us this time around, visit [Symposium’s Mysterious Cove].\n",
            "\n",
            "Contact for parley:\n",
            "Name: [Your trusty informant]\n",
            "Title: [Harbormaster of Information]\n",
            "Bird-mail: [Email]\n",
            "Message in a bottle: [Phone Number]\n",
            "\n",
            "=================\n",
            "FOR IMMEDIATE RELEASE\n",
            "\n",
            "AI Innovators Convene at the Annual Machine Learning Symposium 2023\n",
            "\n",
            "October 18, 2023 — The much-anticipated Annual Machine Learning Symposium 2023 drew to a successful close this past weekend, witnessing groundbreaking discussions and sharing of expertise in the vibrant field of machine learning. The event, held from October 14 to October 16, was an illuminating gathering, marked by the presence of industry experts, scholars, and rising talent.\n",
            "\n",
            "Over 2,000 enthusiasts converged on the Downtown Conference Center in New York City, each contributing a registration fee of $1,450, an investment towards enhancing their understanding and networks in the machine learning sphere. The assembly provided a collaborative platform for participants to forge partnerships, discuss trends, and navigate the complexities of AI technologies.\n",
            "\n",
            "This year's keynote speaker, Dr. Helena Bösch, CTO of Synthetica Industries, delivered an impactful lecture on \"Integrative AI: Melding Machine Learning with Human Intelligence.\" Renowned data scientist Dr. Jonathan Krieger offered fresh perspectives on \"Navigating Data Privacy in Machine Learning,\" a topic resonating with contemporary global concerns. Joining them was the celebrated researcher Dr. Ana-Maria Constantin, shedding light on \"The Evolution of Predictive Analytics in the Post-Pandemic Era.\"\n",
            "\n",
            "One of the symposium's hallmarks was the introduction of a cutting-edge machine learning algorithm designed to revolutionize predictive modeling, unveiled by the trailblazing startup, Prognostica. CEO and founder Dr. Raymond Fok captivated the audience with a live demonstration, showcasing real-world applications and potential industry disruptions.\n",
            "\n",
            "The symposium wasn't solely focused on professional and academic growth. Attendees had the opportunity to participate in various workshops, interactive Q&A sessions, and a highly competitive hackathon sponsored by leading tech enterprise, VirtuTech Solutions. The event also facilitated one-on-one sessions with the speakers, round-table discussions, and evening networking events in a more relaxed setting.\n",
            "\n",
            "In addition, the \"Emergent AI Scholar Award\" and the \"Innovative ML Application of the Year\" were among the accolades presented at the prestigious closing ceremony. These awards acknowledged the contributions of individuals and enterprises propelling the industry forward through innovation, research, and applied solutions.\n",
            "\n",
            "Feedback collected from participants underscored the event's success, pointing to the exceptional organization, high caliber of speakers, diversity of activities, and the invaluable opportunities for networking and skill enhancement.\n",
            "\n",
            "Recordings and resource materials from the event are available for a post-event package fee of $350, providing access to this trove of knowledge for those who couldn’t attend in person. For more information on obtaining these materials or inquiries about the next event, interested parties are encouraged to contact the symposium’s administrative team.\n",
            "\n",
            "Contact Information:\n",
            "Name: Gregory Ashton\n",
            "Title: Public Relations Manager\n",
            "Email: contact@mlsymposium2023.com\n",
            "Phone: +1-202-555-0184\n",
            "\n",
            "About the Annual Machine Learning Symposium:\n",
            "The Annual Machine Learning Symposium is a flagship event in the AI community, dedicated to advancing understanding, fostering talent, and promoting innovation in machine learning, serving as a bridge between groundbreaking technology and real-world application.\n",
            "\n",
            "End of Release\n",
            "\n",
            "=================\n",
            "For Annual Machine Learning Symposium 2023 n_speakers 3 doesn't seem the same as 4\n",
            "Press Release: Cutting-Edge Innovations Debuted at AI Advancements Summit 2023\n",
            "\n",
            "October 18, 2023 — The annual AI Advancements Summit, held on October 16, 2023, drew an enthusiastic crowd of 800 professionals. Registrants secured their participation at $950 per seat, marking the event as one of this year's must-attend meetings in the tech community.\n",
            "\n",
            "The summit showcased groundbreaking AI technologies, with influential thought leaders, Dr. Ana Torres, spotlighting \"Next-Gen Neural Networks,\" and AI strategist Tomás Moralez, unraveling the intricacies behind \"AI and Global Market Economics.\"\n",
            "\n",
            "Amid insightful sessions and networking opportunities, the summit stood out for its blend of academia and industry insights, presenting a 360-degree view of AI's current and future impact.\n",
            "\n",
            "For further details, contact summit@aiadvancements2023.com.\n",
            "\n",
            "End of Release\n",
            "\n",
            "=================\n",
            "Press Release: Innovative Minds Gather at \"AI Horizon 2023\"\n",
            "\n",
            "October 18, 2023 — The highly anticipated AI event of the year, \"AI Horizon 2023,\" was held on October 15, 2023, drawing an impressive crowd of over 2,000 participants. Industry specialists, tech enthusiasts, and students converged to witness and engage in groundbreaking discussions, workshops, and live demonstrations.\n",
            "\n",
            "The hallmark of \"AI Horizon 2023\" was its \"Tech for Good\" segment, where innovations transcended profitability and highlighted societal impact. A charity auction featuring AI-generated artwork and signed memorabilia from keynote speakers was a crowd favorite, successfully raising over $500,000. These funds are earmarked for global educational programs, emphasizing STEM for underprivileged communities.\n",
            "\n",
            "Key discussions led by AI pioneers delved into ethical AI development, sustainability through technology, and inclusivity measures in AI practices. Panels also introduced cutting-edge AI tools poised to revolutionize healthcare, climate change management, and data security.\n",
            "\n",
            "\"AI Horizon 2023\" concluded with a commitment to stronger community collaboration, ethical technological advancement, and the establishment of a grant for aspiring AI startups focusing on humanitarian projects.\n",
            "\n",
            "Details for next year's event and official partnership announcements are expected to be released in the coming weeks.\n",
            "\n",
            "For continuous updates, visit our official website or follow us on social media platforms.\n",
            "\n",
            "End of Release\n",
            "\n",
            "=================\n",
            "For AI Horizon 2023 n_speakers 0 doesn't seem the same as None\n",
            "Press Release: \"AI for Equity Summit\" Champions Inclusivity in Technology\n",
            "\n",
            "October 18, 2023 — The transformative \"AI for Equity Summit\" convened on October 15, 2023, marking an historic congregation of technological prowess dedicated to inclusivity and equitable advancements in artificial intelligence. The event welcomed over 3,000 attendees, each contributing a registration fee of $250, signifying their commitment to nurturing diversity in AI.\n",
            "\n",
            "Six illustrious speakers graced the summit's virtual stage, including Dr. Ayesha Khurram, renowned for her work in ethical AI, tech visionary Omar Svensson, Dr. Lola Adebayo, an advocate for minorities in STEM, Dr. Ji-hoon Park, known for his innovative algorithms against biases, Maria Navarro, a crusader for women in tech, and coding prodigy, Zane Kirschner, who is making significant strides in accessible AI for persons with disabilities.\n",
            "\n",
            "The summit wasn't just about discussions; it was about making tangible strides. The highlight was the launch of the \"AI Equity Initiative,\" a fund that collected over $1.2 million from participant contributions, aimed at supporting tech education in underserved regions.\n",
            "\n",
            "Workshops emphasized actionable strategies for dismantling systemic barriers in the tech industry. Dr. Adebayo's session on 'Intersectionality in AI Development' and Kirschner's workshop titled 'Coding Without Barriers' particularly stood out for their depth and engagement.\n",
            "\n",
            "Moreover, the \"AI for Equity Summit\" was proud to allocate 20% of all registration fees toward scholarships for students from underrepresented backgrounds to pursue AI studies.\n",
            "\n",
            "In the wake of the event, a communiqué was released, expressing a unified pledge among the participants and speakers to proactively include underrepresented groups in both the development of AI and the conversations around it.\n",
            "\n",
            "Attendees and interested parties are encouraged to stay informed through the summit's official channels for ongoing initiatives and future events.\n",
            "\n",
            "End of Release\n",
            "\n",
            "=================\n",
            "Press Release: Revolutionizing Creativity - The \"Generative Intelligence Conclave, Spain 2023\"\n",
            "\n",
            "Madrid, Spain — October 11, 2023 — Groundbreaking innovations were unveiled at the \"Generative Intelligence Conclave, Spain 2023,\" held in Madrid on October 8, attracting over 2,000 enthusiasts, creators, and developers in the field of Generative Artificial Intelligence. The event, priced at €180 per registration, became a melting pot of ideas, setting a new standard in the realm of creative AI technologies.\n",
            "\n",
            "Three keynote speakers, all trailblazers in AI, captivated the audience with their insights. Dr. Francisco José de la Santísima Trinidad Guerrero, an authority in neural networks, delved into the ethics of generative AI. Prof. Maria del Carmen Ángeles de la Concepción Serrano, renowned for her expertise in machine learning, discussed the revolutionary impact of AI on contemporary art and design. Lastly, Dr. Alejandro César del Sagrado Corazón de Jesús Villanueva explored the boundaries of AI in enhancing musical creativity, inspiring many with his team's innovative endeavors.\n",
            "\n",
            "The conclave was more than a series of lectures; it was an interactive experience. Hands-on workshops allowed participants to create their own AI-generated content, from visual arts to symphonic compositions. A special segment of the event showcased AI-generated art, some of which were auctioned. Proceeds, amounting to €30,000, were generously donated to local schools to support technological education for the underprivileged youth.\n",
            "\n",
            "Furthermore, the event saw the launch of a collaborative generative AI community platform, aimed at providing support and continuing innovation among professionals and hobbyists alike.\n",
            "\n",
            "In compliance with environmental standards, the \"Generative Intelligence Conclave\" offset all its carbon footprints, reinforcing its commitment to responsibility as we step into the future of technology.\n",
            "\n",
            "The event closed with a panel discussion featuring the keynote speakers, sparking a forward-thinking dialogue on the limitless potential and ethical considerations of generative AI.\n",
            "\n",
            "For ongoing collaborations and future participations, attendees and AI aficionados can connect through the conclave's official digital platforms.\n",
            "\n",
            "End of Release\n",
            "\n",
            "=================\n",
            "Correctly extracted 32 out of 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Character localization\n",
        "\n",
        "Cool thing about structured output, is that it's very easy to make a translated version of a specific dataset, taking into account all the context and outputing in a format, which is super easy to parse. Let's try this on MMLU.\n",
        "\n",
        "**Task:** Write a function which inputs a sample from MMLU and outputs a translated version, using structured outputs.\n",
        "\n",
        "Tip: make sure that the correct answer didn't change."
      ],
      "metadata": {
        "id": "EDQRdBcLe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "wsZ7lgLXe0DM",
        "outputId": "428f94d5-9025-4928-b76f-55d0efe6e29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m348.2/491.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    ...\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    ..."
      ],
      "metadata": {
        "id": "-iMfgJG7e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    question: str\n",
        "    A: str\n",
        "    B: str\n",
        "    C: str\n",
        "    D: str\n",
        "    correct_answer: str\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this MMLU sample into {target_language}\" \\\n",
        "                f\"Question: {sample.question}\\n\" \\\n",
        "                f\"A: {sample.A}\\n\" \\\n",
        "                f\"B: {sample.B}\\n\" \\\n",
        "                f\"C: {sample.C}\\n\" \\\n",
        "                f\"D: {sample.D}\\n\" \\\n",
        "                f\"Correct answer: {sample.correct_answer}\\n\" \\\n",
        "                f\"Translated sample:\"\n",
        "            }\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": MMLUSample.model_json_schema()\n",
        "        },\n",
        "    )\n",
        "\n",
        "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
        "    if translated.correct_answer != sample.correct_answer:\n",
        "        translated.correct_answer = sample.correct_answer\n",
        "    return translated"
      ],
      "metadata": {
        "id": "eieWNf6Ze0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_sample = MMLUSample(\n",
        "    question = \"Which of the following statements about Ethernets is typically FALSE?\",\n",
        "    A = \"Ethernets use circuit switching to send messages.\",\n",
        "    B = \"Ethernets use buses with multiple masters.\",\n",
        "    C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\",\n",
        "    D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\",\n",
        "    correct_answer = \"A\"\n",
        ")\n",
        "\n",
        "translate_mmlu_sample(mmlu_sample, target_language=\"German\")"
      ],
      "metadata": {
        "id": "6EqyzfJxe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remember the code we've written for MMLU evaluator and add a little twist:\n",
        "\n",
        "We'll have both topic and language in which we want to evaluate the model."
      ],
      "metadata": {
        "id": "uwoVgXi7e0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "KIB_2RFYe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Modify the following MMLUEvaluator code so that it can also translate the input question and evaluate the performance in a different language."
      ],
      "metadata": {
        "id": "TIqzv_AJe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "KmbtCJnNe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "78Eqh6jhe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(topic=\"medical_genetics\", language=\"English\")\n",
        "\n",
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=50)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "DpQ9Lp6_e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_de = MMLUEvaluator(topic=\"medical_genetics\", language=\"German\")\n",
        "\n",
        "results_de = evaluator_de.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=10)\n",
        "print(f'\\nAccuracy: {results_de[\"accuracy\"]}')"
      ],
      "metadata": {
        "id": "-_jrw-z0e0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIW2nMg8e0DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}